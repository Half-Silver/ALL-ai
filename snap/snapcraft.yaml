name: all-ai
base: core24
version: '1.0.0'
summary: ALL ai â€“ private CPU-only AI chat app (llama.cpp server + web UI)
description: |
  ALL ai is a private, CPU-only AI chat Snap built on llama.cpp with its modern
  built-in Web UI. Runs fully local on ALL Core OS (Ubuntu Core based).

  Features:
  - CPU-only inference via llama.cpp
  - Embedded web UI served by the llama-server
  - Default model bootstrap helper (Mistral 7B Instruct v0.2 Q4_K_M)
  - Simple daemon app exposing the UI on a local port

grade: stable
confinement: strict

platforms:
  amd64:
    build-on: amd64
    build-for: amd64
  arm64:
    build-on: arm64
    build-for: arm64

apps:
  server:
    command: bin/start.sh
    daemon: simple
    restart-condition: on-failure
    restart-delay: 5s
    plugs:
      - network
      - network-bind

  all-ai:
    command: bin/all-ai.sh
    plugs:
      - network
      - network-bind

  fetch-model:
    command: bin/fetch_model.sh
    plugs:
      - network

parts:
  # Build llama.cpp and the embedded web UI server (llama-server)
  llama-cpp:
    plugin: cmake
    source: ./backend/llama.cpp
    cmake-parameters:
      - -DCMAKE_BUILD_TYPE=Release
      - -DLLAMA_BUILD_EXAMPLES=ON
      - -DLLAMA_BUILD_SERVER=ON
      - -DLLAMA_BUILD_TESTS=OFF
      - -DLLAMA_CURL=OFF
    build-packages:
      - build-essential
      - cmake
      - pkg-config
      - python3
    build-snaps:
      - node/20/stable
    build-environment:
      - PATH: /snap/bin:$PATH
    stage-packages:
      - libstdc++6
      - libgomp1
    override-build: |
      set -eux
      # Build the Web UI (writes to ../public via the webui build plugin)
      pushd "$CRAFT_PART_SRC/tools/server/webui"
      npm ci
      npm run build
      popd

      # Proceed with the normal cmake build & install (installs llama-server into $CRAFT_PART_INSTALL/usr/bin)
      craftctl default

      # Ensure common llama.cpp tools are present in the snap
      # Try both $CRAFT_PART_BUILD/bin and $CRAFT_PART_BUILD roots depending on upstream layout
      mkdir -p "$CRAFT_PART_INSTALL/usr/bin"
      for f in llama-server main llama-cli quantize llama-quantize embedding embed perplexity llama-bench bench; do
        if [ -f "$CRAFT_PART_BUILD/bin/$f" ]; then
          install -m 0755 "$CRAFT_PART_BUILD/bin/$f" "$CRAFT_PART_INSTALL/usr/bin/$f" || true
        elif [ -f "$CRAFT_PART_BUILD/$f" ]; then
          install -m 0755 "$CRAFT_PART_BUILD/$f" "$CRAFT_PART_INSTALL/usr/bin/$f" || true
        fi
      done

  # Stage our app scripts and helper files
  app-scripts:
    plugin: dump
    source: ..
    stage:
      - bin/*
      - backend/models/*
      - assets/branding/*
    prime:
      - bin/*.sh
      - backend/models/*
      - assets/branding/*
    build-packages:
      - bash
    stage-packages:
      - curl
      - netcat-openbsd
      - bash
      - procps
    override-build: |
      set -eux
      # Perform default dump into $CRAFT_PART_INSTALL
      craftctl default
      # Ensure required launcher scripts exist and are executable
      mkdir -p "$CRAFT_PART_INSTALL/bin"
      for f in start.sh all-ai.sh fetch_model.sh; do
        if [ ! -f "$CRAFT_PART_INSTALL/bin/$f" ]; then
          if [ -f "${CRAFT_PROJECT_DIR:-}/bin/$f" ]; then
            install -D -m 0755 "${CRAFT_PROJECT_DIR:-}/bin/$f" "$CRAFT_PART_INSTALL/bin/$f"
          elif [ -f "$CRAFT_PART_SRC/bin/$f" ]; then
            install -D -m 0755 "$CRAFT_PART_SRC/bin/$f" "$CRAFT_PART_INSTALL/bin/$f"
          else
            echo "Warning: launcher script not found: $f" >&2
          fi
        fi
      done
    override-prime: |
      set -eux
      craftctl default
      # Make startup scripts executable if present
      if [ -d "$CRAFT_PRIME/bin" ]; then
        if ls "$CRAFT_PRIME/bin/"*.sh >/dev/null 2>&1; then
          chmod +x "$CRAFT_PRIME"/bin/*.sh
        fi
      fi
environment:
  PATH: $SNAP/usr/bin:$SNAP/bin:$PATH
  LANG: C.UTF-8
  LC_ALL: C.UTF-8
  ALLAI_DEFAULT_MODEL: $SNAP_COMMON/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf
